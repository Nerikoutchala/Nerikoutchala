{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONp4tIXBccmyqvuk1dOduX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##                                      **PRACTICA BIG DATA II**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "  Alumno: Neri KOUTCHALA\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###                                    CLASIFICACIÓN NON BALANCEADA\n",
        "\n"
      ],
      "metadata": {
        "id": "qsm99HSMz6Xc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. INSTALAMOS EL ENTORNO SPARK**"
      ],
      "metadata": {
        "id": "QnssP-MHwZGX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw_HyDgB-5Tl",
        "outputId": "213b79b5-e572-472a-e619-18e1c5cf3f0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=7dccbfb075bb15a7a9da569ec5afd860782d9ab55e825ad2d0779e110787802c\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "#Primero instalamos Apache Spark con Hadoop\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.3/spark-3.2.3-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.2.3-bin-hadoop2.7.tgz \n",
        "\n",
        "#Instalamos los paquetes de Python para trabajar con Spark\n",
        "!pip install findspark #Instalamos FindSpark\n",
        "!pip install pyspark   #Instalamos Spark\n",
        "\n",
        "#Indicamos a PySpark donde está Spark\n",
        "import findspark\n",
        "findspark.init(\"spark-3.2.3-bin-hadoop2.7\")#SPARK_HOME\n",
        "\n",
        "#Inicializamos las variables de entorno\n",
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.3-bin-hadoop2.7\"\n",
        "\n",
        "\n",
        "\n",
        "#Creamos una sesión de Spark para poder trabajar\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Clasificación non Balanceada con PySpark\") \\\n",
        "    .getOrCreate() "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. CARGA DE LAS BIBLIOTECAS** "
      ],
      "metadata": {
        "id": "GN2AAKhjAuLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "ymS7EWGkAzrd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. DESCARGAMOS, CARGAMOS Y PROCESAMOS LOS DATOS**"
      ],
      "metadata": {
        "id": "9nElUv1o_npp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Descargamos los ficheros de datos\n",
        "!wget -nv --no-check-certificate 'https://docs.google.com/uc?export=download&id=1HOrM49tCLA_NqHyD_ps_cv483FPN7aWo' -O susy-10k-tra.csv\n",
        "!wget -nv --no-check-certificate 'https://docs.google.com/uc?export=download&id=1HT80d5cwU7HMi2XK8CNgxgHvxRZEZB_d' -O susy-10k-tst.csv\n",
        "\n",
        "#Leemos los conjuntos de entrenamiento y test\n",
        "dfTra = spark.read.csv('susy-10k-tra.csv', inferSchema=True, header=True)\n",
        "dfTst = spark.read.csv('susy-10k-tst.csv', inferSchema=True, header=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhVAcoN7_iOD",
        "outputId": "12b04f53-9b61-44ad-d7ec-7ff5c63f58e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-22 08:34:34 URL:https://doc-08-bc-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qtfftouitt30nlhoalvqcsoiatmnp8g5/1684744425000/11180625338828972622/*/1HOrM49tCLA_NqHyD_ps_cv483FPN7aWo?e=download&uuid=7b44dd42-7dcb-415e-ad34-a63a42f13e04 [3463157/3463157] -> \"susy-10k-tra.csv\" [1]\n",
            "2023-05-22 08:34:35 URL:https://doc-14-bc-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/imcr599loob8a4acqrfuktjld25ss7be/1684744425000/11180625338828972622/*/1HT80d5cwU7HMi2XK8CNgxgHvxRZEZB_d?e=download&uuid=1b8bce1e-12ab-4570-9d16-964fa3479094 [3469204/3469204] -> \"susy-10k-tst.csv\" [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convertimos la variable clase en entera\n",
        "trainData=dfTra.withColumn(\"clase\",dfTra.clase.cast(\"Integer\"))\n",
        "testData=dfTst.withColumn(\"clase\",dfTst.clase.cast(\"Integer\"))"
      ],
      "metadata": {
        "id": "yI_DvprDao3r"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainData.show(5)\n",
        "testData.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TEHuSPLcNOQ",
        "outputId": "dbb43cbf-065d-49be-fb14-de2491dbc48d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-------------------+--------------------+-------------------+--------------------+-------------------+-------------------+--------------------+-------------------+--------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+-----+\n",
            "|               uno|                dos|                tres|             cuatro|               cinco|               seis|              siete|                ocho|              nueve|                diez|              once|              doce|             trece|            catorce|            quince|         dieciseis|        diecisiete|          dieciocho|clase|\n",
            "+------------------+-------------------+--------------------+-------------------+--------------------+-------------------+-------------------+--------------------+-------------------+--------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+-----+\n",
            "|0.6433419585227966| 1.3615427017211914|  0.6828370690345764|0.48453789949417114|-0.32587578892707825|-0.8524075746536255|  1.145269513130188|  0.8196632862091064|0.40248867869377136|   1.325475811958313|0.7232717275619507|0.6548330783843994|0.8034266233444214|0.37070924043655396|0.7308408617973328|0.5756281018257141|1.4113106727600098|0.19226300716400146|    1|\n",
            "|1.2374616861343384|-0.4839438199996948|  1.3800559043884277| 1.6685774326324463|  0.4818389117717743|-0.6302737593650818| 0.6783822178840637|-0.43130290508270264|0.34755226969718933|-0.01465716026723...|1.3650861978530884| 1.040429711341858|0.6763473749160767| 0.6026328206062317|  1.36842942237854|0.7144116759300232|0.9274933934211731|0.16320399940013885|    1|\n",
            "|1.3433129787445068|  0.783230721950531| -0.7952934503555298|  1.475182294845581|  0.6200259327888489|0.43646135926246643| 0.8689783811569214|   1.131447672843933| 0.3842912018299103|-0.49663621187210083|1.1656821966171265|1.3288121223449707|1.0115764141082764| 1.8998759984970093|1.1543011665344238| 1.430264949798584|0.6719890236854553|0.28056100010871887|    0|\n",
            "|0.6531051993370056| 0.9232335686683655|-0.31942081451416016|  0.519090473651886|-0.06831558048725128|-1.6212221384048462|0.38024044036865234|  0.8473420143127441|0.30507954955101013|-0.00403195200487...|0.5825600624084473|0.5741020441055298|0.8745092153549194| 0.8703949451446533| 0.573835015296936|0.6802302598953247|0.6731339693069458|0.19332100450992584|    0|\n",
            "|1.5516648292541504|-0.4539290964603424|  1.4226588010787964|  2.352872848510742|  0.7626277804374695|0.03150284290313721| 1.0506685972213745| -0.7840972542762756|  1.571985125541687| -0.7112254500389099|1.9744004011154175|1.5981488227844238|0.7182844281196594| 2.3227782249450684|1.9981735944747925|1.7646944522857666|   1.2172771692276|0.06346789747476578|    1|\n",
            "+------------------+-------------------+--------------------+-------------------+--------------------+-------------------+-------------------+--------------------+-------------------+--------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------------------+-------------------+--------------------+-------------------+--------------------+-------------------+------------------+-------------------+------------------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+--------------------+-----+\n",
            "|                uno|                dos|                tres|             cuatro|               cinco|               seis|             siete|               ocho|             nueve|                diez|              once|              doce|             trece|           catorce|            quince|         dieciseis|        diecisiete|           dieciocho|clase|\n",
            "+-------------------+-------------------+--------------------+-------------------+--------------------+-------------------+------------------+-------------------+------------------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+--------------------+-----+\n",
            "| 1.6795326471328735|-1.3285973072052002|   1.721287727355957| 0.8961411714553833|  0.7851014137268066| -0.300307035446167|2.4243409633636475|0.24690665304660797| 3.021253824234009| -2.0464718341827393|1.9866414070129395|2.2912914752960205|1.0234774351119995|2.0351436138153076| 2.139514207839966| 2.923895835876465|1.4491009712219238|  0.2602680027484894|    1|\n",
            "| 0.8576836585998535|0.10346849262714386|-0.15959890186786652|0.49158814549446106|-0.46780043840408325| 1.6336811780929565|1.2118911743164062| 1.1086699962615967| 1.492547631263733| -0.5306073427200317|0.6292113065719604|1.0887541770935059|1.5354979038238525|0.5368104577064514|0.7166863679885864|0.8678085207939148|1.2548242807388306|  0.5520399808883667|    1|\n",
            "|  0.679413378238678| 0.6467429399490356| -0.7623209953308105| 1.0956929922103882| 0.04633910581469536| 1.1650936603546143|1.0221043825149536| -1.074870228767395|0.8398934602737427|  1.1044260263442993|0.7407406568527222|0.8633846044540405|1.0343234539031982|               0.0| 0.770002007484436|0.4708144962787628|1.4442930221557617| 0.24737399816513062|    1|\n",
            "| 0.7668826580047607| 0.4359167516231537|  -1.404170036315918| 0.7847794890403748| -0.8897899389266968|-0.7484598755836487|0.8655236959457397| 0.8442953824996948|1.2992560863494873| -0.4516412317752838| 0.851235032081604| 1.084952712059021| 1.131034255027771|2.2521724700927734|0.8331539630889893|1.6848599910736084|0.7220147848129272| 0.09403660148382187|    1|\n",
            "|0.48874709010124207| 0.4925791323184967|  1.6006025075912476| 0.6941078305244446| -1.8233418464660645| 0.4462366998195648|0.4422283172607422|-0.9721801280975342|0.6638392210006714|-0.04252281785011...|1.0111192464828491| 0.596332311630249|0.5233632922172546|1.0546964406967163|1.0142439603805542|0.7886103987693787|0.8621771931648254|0.012908900156617165|    0|\n",
            "+-------------------+-------------------+--------------------+-------------------+--------------------+-------------------+------------------+-------------------+------------------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "\n",
        "# Definimos los nombres de las columnas input y output\n",
        "input_cols = trainData.columns[:-1]\n",
        "output_col = \"atributos\"\n",
        " \n",
        "#Unimos las variables de entrada con VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=trainData.columns[:-1],outputCol=\"atributos\")\n",
        "train = assembler.transform(trainData)\n",
        "train.select('atributos','clase').show(5)\n",
        "\n",
        "#Uso el mismo VectorAssemble para el conjunto de test (las columnas son iguales)\n",
        "test = assembler.transform(testData)\n",
        "test.select('atributos','clase').show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3rxXvrccqAL",
        "outputId": "fa0ceb4a-c162-4b52-95c3-6f4d2d6bae00"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|           atributos|clase|\n",
            "+--------------------+-----+\n",
            "|[0.64334195852279...|    1|\n",
            "|[1.23746168613433...|    1|\n",
            "|[1.34331297874450...|    0|\n",
            "|[0.65310519933700...|    0|\n",
            "|[1.55166482925415...|    1|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+--------------------+-----+\n",
            "|           atributos|clase|\n",
            "+--------------------+-----+\n",
            "|[1.67953264713287...|    1|\n",
            "|[0.85768365859985...|    1|\n",
            "|[0.67941337823867...|    1|\n",
            "|[0.76688265800476...|    1|\n",
            "|[0.48874709010124...|    0|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. DEFINIMOS EL TRATAMIENTO ROS Y RUS**"
      ],
      "metadata": {
        "id": "gRd2-RtcA9v0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "\n",
        "# Convertir el DataFrame de Spark en DataFrame de Pandas\n",
        "train_data_pd = train.toPandas()\n",
        "\n",
        "# Aplicar el algoritmo ROS para equilibrar los datos de formación\n",
        "ros = RandomOverSampler()\n",
        "train_data_balanced, _ = ros.fit_resample(train_data_pd[input_cols], train_data_pd[\"clase\"])\n",
        "train_data_balanced = pd.concat([train_data_balanced, train_data_pd[\"clase\"]], axis=1)\n",
        "\n",
        "# Eliminar filas con valores NaN\n",
        "train_data_balanced = train_data_balanced.dropna()\n",
        "\n",
        "# Aplicar el algoritmo RUS para equilibrar aún más los datos de entrenamiento\n",
        "rus = RandomUnderSampler()\n",
        "train_data_balanced, _ = rus.fit_resample(train_data_balanced[input_cols], train_data_balanced[\"clase\"])\n",
        "train_data_balanced = pd.concat([train_data_balanced, train_data_pd[\"clase\"]], axis=1)\n",
        "\n",
        "# Crear un nuevo Spark DataFrame a partir del Pandas DataFrame equilibrado\n",
        "train_data_balanced = spark.createDataFrame(train_data_balanced)\n",
        "\n",
        "# Transforma los datos de entrenamiento equilibrados con VectorAssembler\n",
        "train_data_balanced = assembler.transform(train_data_balanced)\n",
        "\n",
        "# Mostrar los datos de entrenamiento equilibrados\n",
        "train_data_balanced.select(output_col, \"clase\").show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-yWY7BWEiL3",
        "outputId": "6c8edc31-38c6-442b-964d-2b0015aaf13b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:371: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
            "  for column, series in pdf.iteritems():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|           atributos|clase|\n",
            "+--------------------+-----+\n",
            "|[0.66922366619110...|    1|\n",
            "|[0.75525563955307...|    1|\n",
            "|[2.45638990402221...|    0|\n",
            "|[0.30020308494567...|    0|\n",
            "|[0.63320702314376...|    1|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WKk80pfDbP__"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. DEFINIMOS LAS ETAPAS Y LA TUBERÍA**"
      ],
      "metadata": {
        "id": "BUWpvPT9Exr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Unimos las variables de entrada con VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=train.columns[:-1],outputCol=\"features\")\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "\n",
        "#label_indexer = StringIndexer(inputCol=output_col, outputCol=\"clase\")\n",
        "lr = LogisticRegression(featuresCol=\"scaledFeatures\", labelCol=\"clase\")\n",
        "\n",
        "#Añadimos el preprocesamiento a una tubería\n",
        "from pyspark.ml import Pipeline\n",
        "tuberiaParcial = Pipeline().setStages([assembler, scaler,  lr])\n",
        "\n",
        "#Construimos el modelo\n",
        "model = tuberiaParcial.fit(train)\n",
        "\n",
        "#Transformamos los datos y los mostramos\n",
        "datosPreprocesados = model.transform(train)\n",
        "datosPreprocesados.select('features','clase').show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJEWUEFFpnJv",
        "outputId": "eb9a7132-ac7e-4b2a-afab-54c440efa7a8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|            features|clase|\n",
            "+--------------------+-----+\n",
            "|[0.64334195852279...|    1|\n",
            "|[1.23746168613433...|    1|\n",
            "|[1.34331297874450...|    0|\n",
            "|[0.65310519933700...|    0|\n",
            "|[1.55166482925415...|    1|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. EVALUACIÓN DEL MODELO**"
      ],
      "metadata": {
        "id": "4e-ktPJHkmri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Aplicar el modelo al conjunto de datos de prueba para generar predicciones\n",
        "predictions = model.transform(test)\n",
        "\n",
        "# Evaluar el modelo\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"clase\")\n",
        "area_under_roc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
        "\n",
        "# Calcular la tasa de verdaderos positivos (TPR)\n",
        "tp = predictions.filter(\"prediction = 1.0 AND clase = 1.0\").count()\n",
        "fn = predictions.filter(\"prediction = 0.0 AND clase = 1.0\").count()\n",
        "tpr = tp / (tp + fn)\n",
        "\n",
        "# Calcular la tasa de verdaderos negativos (TNR)\n",
        "tn = predictions.filter(\"prediction = 0.0 AND clase = 0.0\").count()\n",
        "fp = predictions.filter(\"prediction = 1.0 AND clase = 0.0\").count()\n",
        "tnr = tn / (tn + fp)\n",
        "\n",
        "# Calcular el producto TPR x TNR\n",
        "tpr_tnr_product = tpr * tnr\n",
        "\n",
        "# Imprimir los resultados de la evaluación\n",
        "print(\"Area under ROC:\", area_under_roc)\n",
        "print(\"True Positive Rate (TPR):\", tpr)\n",
        "print(\"True Negative Rate (TNR):\", tnr)\n",
        "print(\"TPR x TNR Product:\", tpr_tnr_product)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCqu5nj-pnbN",
        "outputId": "ed7fb3f9-ea07-4da3-8117-ffb0b91a36c5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area under ROC: 0.9999998208691394\n",
            "True Positive Rate (TPR): 1.0\n",
            "True Negative Rate (TNR): 1.0\n",
            "TPR x TNR Product: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. APLICACIÓN DEL APRENDIZAJE AUTOMÁTICO**"
      ],
      "metadata": {
        "id": "ghozRI7v381z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, LogisticRegression, LinearSVC\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Definir los nombres de las columnas de entrada y salida\n",
        "input_col = \"atributos\"\n",
        "output_col = \"features\"\n",
        "\n",
        "# Definir los clasificadores\n",
        "classifiers = [\n",
        "    DecisionTreeClassifier(featuresCol=output_col, labelCol='clase'),\n",
        "    RandomForestClassifier(featuresCol=output_col, labelCol='clase'),\n",
        "    GBTClassifier(featuresCol=output_col, labelCol='clase'),\n",
        "    LogisticRegression(featuresCol=output_col, labelCol='clase'),\n",
        "    LinearSVC(featuresCol=output_col, labelCol='clase')\n",
        "]\n",
        "\n",
        "# Definir el evaluador\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"clase\", metricName=\"accuracy\")\n",
        "\n",
        "# Crear una lista vacía para almacenar los resultados de la evaluación\n",
        "results = []\n",
        "\n",
        "# Iterar sobre los clasificadores\n",
        "for classifier in classifiers:\n",
        "    # Crear el VectorAssembler para cada clasificador\n",
        "    assembler = VectorAssembler(inputCols=[input_col], outputCol=\"vectorized_features\")\n",
        "\n",
        "    # Crear el MinMaxScaler para cada clasificador\n",
        "    scaler = MinMaxScaler(inputCol=\"vectorized_features\", outputCol=output_col)\n",
        "\n",
        "    # Crear el pipeline para cada clasificador\n",
        "    pipeline = Pipeline(stages=[assembler, scaler, classifier])\n",
        "\n",
        "    # Ajustar el pipeline a los datos de entrenamiento\n",
        "    model = pipeline.fit(train)\n",
        "\n",
        "    # Aplicar la canalización a los datos de prueba\n",
        "    predictions = model.transform(test)\n",
        "\n",
        "    # Evaluar las métricas\n",
        "    accuracy = evaluator.evaluate(predictions)\n",
        "    f1_score = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
        "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "    # Almacenar los resultados de la evaluación\n",
        "    results.append([classifier.__class__.__name__, accuracy, f1_score, precision, recall])\n",
        "\n",
        "# Preparar las cabeceras de las tablas\n",
        "headers = [\"Classifier\", \"Accuracy\", \"F1 Score\", \"Precision\", \"Recall\"]\n",
        "\n",
        "# Imprimir los resultados de la evaluación en forma de tabla\n",
        "print(tabulate(results, headers=headers, tablefmt=\"grid\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yG-mz19AKS2L",
        "outputId": "dc6e1097-6135-4b84-d6ed-c28d994fcafe"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------+------------+------------+-------------+----------+\n",
            "| Classifier             |   Accuracy |   F1 Score |   Precision |   Recall |\n",
            "+========================+============+============+=============+==========+\n",
            "| DecisionTreeClassifier |     0.7876 |   0.786893 |    0.786315 |   0.7876 |\n",
            "+------------------------+------------+------------+-------------+----------+\n",
            "| RandomForestClassifier |     0.7937 |   0.793975 |    0.794273 |   0.7937 |\n",
            "+------------------------+------------+------------+-------------+----------+\n",
            "| GBTClassifier          |     0.7964 |   0.797172 |    0.798137 |   0.7964 |\n",
            "+------------------------+------------+------------+-------------+----------+\n",
            "| LogisticRegression     |     0.8081 |   0.807603 |    0.807187 |   0.8081 |\n",
            "+------------------------+------------+------------+-------------+----------+\n",
            "| LinearSVC              |     0.8128 |   0.8111   |    0.810316 |   0.8128 |\n",
            "+------------------------+------------+------------+-------------+----------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. VALIDACIÓN**"
      ],
      "metadata": {
        "id": "j2mQv7sfSWgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, LogisticRegression, LinearSVC\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "\n",
        "# Fusionar los conjuntos de datos de entrenamiento y de prueba en un único marco de datos\n",
        "df = dfTra.union(dfTst)\n",
        "\n",
        "# Crear un EvaluadorClasificaciónMulticlase\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"clase\", metricName=\"accuracy\")\n",
        "\n",
        "# Definir los clasificadores\n",
        "classifiers = [\n",
        "    DecisionTreeClassifier(featuresCol=\"atributos\", labelCol='clase'),\n",
        "    RandomForestClassifier(featuresCol=\"atributos\", labelCol='clase'),\n",
        "    GBTClassifier(featuresCol=\"atributos\", labelCol='clase'),\n",
        "    LogisticRegression(featuresCol=\"atributos\", labelCol='clase'),\n",
        "    LinearSVC(featuresCol=\"atributos\", labelCol='clase')\n",
        "]\n",
        "\n",
        "# Crear una lista vacía para almacenar los resultados de la validación cruzada\n",
        "results = []\n",
        "\n",
        "# Realizar una validación cruzada para cada clasificador\n",
        "for classifier in classifiers:\n",
        "    # Definir la malla de parámetros para la validación cruzada\n",
        "    param_grid = ParamGridBuilder().build()\n",
        "\n",
        "    # Crear el CrossValidator\n",
        "    crossval = CrossValidator(estimator=classifier,\n",
        "                              estimatorParamMaps=param_grid,\n",
        "                              evaluator=evaluator,\n",
        "                              numFolds=10)\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    cvModel = crossval.fit(df)\n",
        "\n",
        "    # Almacenar la precisión del modelo\n",
        "    accuracy = cvModel.avgMetrics[0]\n",
        "    results.append([classifier.__class__.__name__, accuracy])\n",
        "\n",
        "# Preparar las cabeceras de las tablas\n",
        "headers = [\"Classifier\", \"Accuracy\"]\n",
        "\n",
        "# Imprimir los resultados de validación cruzada en forma de tabla\n",
        "print(tabulate(results, headers=headers, tablefmt=\"grid\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuiFanmHSbHE",
        "outputId": "f874d780-6ef9-49b4-99d1-c050600d0c10"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------+------------+\n",
            "| Classifier             |   Accuracy |\n",
            "+========================+============+\n",
            "| DecisionTreeClassifier |   0.767222 |\n",
            "+------------------------+------------+\n",
            "| RandomForestClassifier |   0.769482 |\n",
            "+------------------------+------------+\n",
            "| GBTClassifier          |   0.778637 |\n",
            "+------------------------+------------+\n",
            "| LogisticRegression     |   0.778426 |\n",
            "+------------------------+------------+\n",
            "| LinearSVC              |   0.776427 |\n",
            "+------------------------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. AJUSTE DE LOS HIPERPARÁMETROS DE LOS MODELOS**"
      ],
      "metadata": {
        "id": "jyfKrA98942_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un EvaluadorClasificaciónMulticlase\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"clase\", metricName=\"accuracy\")\n",
        "\n",
        "# Define the classifiers\n",
        "classifiers = [\n",
        "    DecisionTreeClassifier(featuresCol=\"atributos\", labelCol='clase'),\n",
        "    RandomForestClassifier(featuresCol=\"atributos\", labelCol='clase'),\n",
        "    GBTClassifier(featuresCol=\"atributos\", labelCol='clase'),\n",
        "    LogisticRegression(featuresCol=\"atributos\", labelCol='clase'),\n",
        "    LinearSVC(featuresCol=\"atributos\", labelCol='clase')\n",
        "]\n",
        "\n",
        "# Crear una lista vacía para almacenar los resultados de la validación cruzada\n",
        "results = []\n",
        "\n",
        "# Ajuste de los hiperparámetros de cada clasificador\n",
        "for classifier in classifiers:\n",
        "    if isinstance(classifier, DecisionTreeClassifier):\n",
        "        paramGrid = ParamGridBuilder() \\\n",
        "            .addGrid(classifier.maxDepth, [5, 10, 20]) \\\n",
        "            .build()\n",
        "    elif isinstance(classifier, RandomForestClassifier):\n",
        "        paramGrid = ParamGridBuilder() \\\n",
        "            .addGrid(classifier.numTrees, [10, 5, 10]) \\\n",
        "            .addGrid(classifier.maxDepth, [5, 10, 20]) \\\n",
        "            .build()\n",
        "    elif isinstance(classifier, GBTClassifier):\n",
        "        paramGrid = ParamGridBuilder() \\\n",
        "            .addGrid(classifier.maxIter, [10, 5, 10]) \\\n",
        "            .addGrid(classifier.maxDepth, [5, 10, 20]) \\\n",
        "            .build()\n",
        "    elif isinstance(classifier, LogisticRegression):\n",
        "        paramGrid = ParamGridBuilder() \\\n",
        "            .addGrid(classifier.maxIter, [10, 10, 10]) \\\n",
        "            .addGrid(classifier.regParam, [0.1, 0.01]) \\\n",
        "            .build()\n",
        "    elif isinstance(classifier, LinearSVC):\n",
        "        paramGrid = ParamGridBuilder() \\\n",
        "            .addGrid(classifier.maxIter, [10, 10, 10]) \\\n",
        "            .addGrid(classifier.regParam, [0.1, 0.01]) \\\n",
        "            .build()\n",
        "\n",
        "    # Crear el CrossValidator\n",
        "    crossval = CrossValidator(estimator=classifier,\n",
        "                              estimatorParamMaps=paramGrid,\n",
        "                              evaluator=evaluator,\n",
        "                              numFolds=10)\n",
        "\n",
        "    # Entrenar el modelo con el conjunto de entrenamiento\n",
        "    cvModel = crossval.fit(train)\n",
        "\n",
        "    # Almacenar las métricas de evaluación\n",
        "    metrics = cvModel.avgMetrics\n",
        "    results.append([classifier.__class__.__name__, metrics])\n",
        "\n",
        "# Preparar las cabeceras de las tablas\n",
        "headers = [\"Classifier\", \"Metrics\"]\n",
        "\n",
        "# Imprimir los resultados del ajuste de hiperparámetros en forma de tabla\n",
        "print(tabulate(results, headers=headers, tablefmt=\"grid\"))\n",
        "\n",
        "# Utilizamos el conjunto de pruebas para medir el rendimiento del modelo\n",
        "# utilizando la mejor configuración (bestModel) de los parámetros encontrados\n",
        "predictions = cvModel.bestModel.transform(test)\n",
        "\n",
        "# Show the accuracy of the model\n",
        "print('Accuracy:', evaluator.evaluate(predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwETR2WE5Qj-",
        "outputId": "f17e869a-9f82-4dce-f9bd-3dba670b9069"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Classifier             | Metrics                                                                                                                                                                             |\n",
            "+========================+=====================================================================================================================================================================================+\n",
            "| DecisionTreeClassifier | [0.7672221627102239, 0.7502179427134458, 0.6952908613645221]                                                                                                                        |\n",
            "+------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| RandomForestClassifier | [0.7660490488144309, 0.7708048686000801, 0.7603982999478875, 0.7645985169293043, 0.7665021452400167, 0.747649097554034, 0.7660490488144309, 0.7708048686000801, 0.7603982999478875] |\n",
            "+------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| GBTClassifier          | [0.776078999826573, 0.7533070630664249, 0.6949898570271238, 0.7708365945466248, 0.7520295169019315, 0.6950847204316756, 0.776078999826573, 0.7533070630664249, 0.6949898570271238]  |\n",
            "+------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| LogisticRegression     | [0.7572786998184118, 0.7713769526362982, 0.7572786998184118, 0.7713769526362982, 0.7572786998184118, 0.7713769526362982]                                                            |\n",
            "+------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| LinearSVC              | [0.7553445078279929, 0.7685812202603542, 0.7553445078279929, 0.7685812202603542, 0.7553445078279929, 0.7685812202603542]                                                            |\n",
            "+------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Accuracy: 0.8066\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detener la Session Spark\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "b4T05vBVkiPA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}