{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nerikoutchala/Nerikoutchala/blob/main/BigData_MLlib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqfUz-Q1fZJK"
      },
      "source": [
        "## **Big Data II - Prácticas**\n",
        "### **MLlib**\n",
        "\n",
        "**Por: Fco. Javier García Castellano**.\n",
        "\n",
        "*Profesor Titular de Universidad. Departamento de Ciencias de Computación e Inteligencia Artificial (DECSAI). Universidad de Granada.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phaga9kDY6AF"
      },
      "source": [
        "## **ÍNDICE**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny9tgvVapCqc"
      },
      "source": [
        "En este *notebook*: \n",
        "1. Empezaremos a conoce la parte de Machine Learning de Apache Spark:MLlib.\n",
        "2. Veremos como preparar los datos para que un modelo pueda aprenderlos.\n",
        "3. Aprenderemos a utilizar las tubería para optimizar el flujo de trabajo. \n",
        "4. Veremos como validar en Spark los modelos aprendidos\n",
        "\n",
        "Contenidos:\n",
        "1. Introducción. \n",
        "\n",
        "2. Preparar los DataFrame para Spark ML. \n",
        "  \n",
        "3. Tuberías de Spark (Pipelines).\n",
        "\n",
        "4. Tuberías con Machine Learning.\n",
        "\n",
        "5. Métricas para la evaluación de los modelos de aprendizaje supervisado.\n",
        "\n",
        "6. Validación de los modelos de aprendizaje supervisado.\n",
        "\n",
        "7. Ajuste de los hiperparámetros de los modelos. \n",
        "\n",
        "8. Referencias Biobliográficas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGeuySu9X9xU"
      },
      "source": [
        "##**1. INTRODUCCIÓN.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aosumELxpCqc"
      },
      "source": [
        "MLlib es la biblioteca de Machine Learning de Apache Spark. Lo interesante de MLlib, es que los algoritmos de MLlib están pensados para entornos distribuidos, son altamente escalables y con tolerancia a fallos, pues están basados en Spark.\n",
        "\n",
        "Proporciona las siguientes herramientas:\n",
        "\n",
        "*   Algoritmos de Machine Learning: algoritmos comunes de clasificación, regresión, clustering o filtros colaborativos.\n",
        "*   Herramientas de preprocesamiento de datos: herramienta para extraer, seleccionar o transformar las variables.\n",
        "*   Tuberías (Pipelines): es una herramienta para optimizar los flujos de trabajo en Machine Learning.\n",
        "*   Persistencia: para guardar y leer algoritmos, modelos y tuberías.\n",
        "*   Utilidades:  álgebra lineal, estadística, manejo de datos, etc.\n",
        "\n",
        "\n",
        "Cuando usamos Python, dentro de MLlib podemos encontrar dos interfaces de programación:\n",
        "* [Spark ML](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html ) (o [DataFrame MLlib API](https://spark.apache.org/docs/latest/mllib-guide.html#mllib-main-guide)): Que está basada en DataFrames y que está dentro del paquete `spark.ml`. Desde la versión 2.0 esta la versión por defecto y la que se aconseja usar. \n",
        "* [Spark MLlib](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html ) (o [RDD MLlib API](https://spark.apache.org/docs/latest/mllib-guide.html#mllib-rdd-based-api-guide)): Es la interfaz de programación original y está basada en RDD. Está dentro del paquete `spark.mllib`. Actualmente está en modo mantenimiento.\n",
        "\n",
        "El cambio de pasar de API basada en RDDs a otra basada en DataFrames, se debe a que, como hemos visto, éstos últimos son más rápidos y fáciles de manejar.\n",
        "\n",
        "En este cuaderno (notebook) vamos a seguir trabajando con Spark en Google Colab, por lo que los prolegómenos son los mismos, como se muestran a continuación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wf76hHtnAgS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74376519-cae9-4d0c-e130-4e96b866f3ff"
      },
      "source": [
        "#Primero instalamos Apache Spark con Hadoop\n",
        "!wget -q http://www-eu.apache.org/dist/spark/spark-3.2.3/spark-3.2.3-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.2.3-bin-hadoop2.7.tgz \n",
        "\n",
        "#Instalamos los paquetes de Python para trabajar con Spark\n",
        "!pip install findspark #Instalamos FindSpark\n",
        "!pip install pyspark   #Instalamos Spark\n",
        "\n",
        "#Indicamos a PySpark donde está Spark\n",
        "import findspark\n",
        "findspark.init(\"spark-3.2.3-bin-hadoop2.7\")#SPARK_HOME\n",
        "\n",
        "#Inicializamos las variables de entorno\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/default-java\" \n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.3-bin-hadoop2.7\"\n",
        "\n",
        "\n",
        "\n",
        "#Creamos una sesión de Spark para poder trabajar\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Ejemplo Machine Learning PySpark\") \\\n",
        "    .getOrCreate()  "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824025 sha256=764ee534e65c77a15739bb50fcfc449df997ae312ece611b3c53ec936ad66199\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/e3/9b/0525ce8a69478916513509d43693511463c6468db0de237c86\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmYO6PH8fJbO"
      },
      "source": [
        "En un proyecto de Ciencia de Datos normalmente implica preprocesar los datos, selección de variables de entrada, aprendizaje de un modelo y evaluación de los resultados, lo que\n",
        "denominamos el **ciclo de vida** de la Ciencia de Datos. Lo que vamos a ver en esta cápsula es como llevar a cabo dicho ciclo de vida mediante Apache Spark. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5E3Cde2B7Mk"
      },
      "source": [
        "##**2. PREPARAR LOS DATAFRAMES PARA SPARK ML.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_xiNPFhpCqh"
      },
      "source": [
        "Para poder trabajar con los algoritmos de Machine Learning de Spark, tendremos que preparar los datos para que puedan ser procesados. Es más, los algoritmos de Machine Learning de Spark ML, trabajan sólo con **datos en formato numérico**. Por tanto, es esencial convertir a numérica, cualquier variable categórica que esté en nuestro conjunto datos. \n",
        "\n",
        "Veamos algunos de los métodos que se pueden usar para transformar variables:\n",
        "\n",
        "* [`StringIndexer`](http://spark.apache.org/docs/latest/ml-features.html#stringindexer): Asigna un número entero a cada posible categoría. Se asigna el valor cero a la categoría más frecuente 1 a la siguiente y así sucesivamente. Tenemos que indicarle la columna que queremos indexar y el nombre de la columna de salida. En los metadatos se guarda el esquema usado por si queremos deshacer este paso con el método [`IndexToString()`](http://spark.apache.org/docs/latest/ml-features.html#indextostring).\n",
        "\n",
        "\n",
        "* [`OneHotEncoderEstimator`](http://spark.apache.org/docs/latest/ml-features.html#onehotencoderestimator): Crea una columna para cada valor distinto que exista en el atributo que estamos codificando y, para cada instancia, marcar con un 1 la columna a la que pertenezca dicho registro y dejar las demás con 0.  En Spark no se puede pasar directamente una variable categórica a `OneHotEncoderEstimator` sino que tendremos que convertirla a índices previamente con `StringIndexer`. \n",
        "\n",
        "   <!--Esta codificación se usará en aquellos algoritmos que esperan atributos continuos, como por ejemplo la regresion logística. No obstante, la representación one-hot no tendremos que usarla en los atributos categóricos, después de haber usado StringIndexer, cuando vayamos a usar modelos que sí aceptan variables categóricas, como por eemplo, los árboles de decisión.-->\n",
        "\n",
        "*  [`Vector Assembler`](http://spark.apache.org/docs/latest/ml-features.html#vectorassembler): Nos permite combinar una lista dada de columnas en una sola columna vector. Esto es debido a que los algoritmos de Machine Learning aceptan un DataFrame donde las variables de entrada están en una sola columna y, si se trata de aprendizaje supervisado, habrá además una segunda columna con la variable de salida. \n",
        "\n",
        "  Este paso se produce normalmente **al final del preprocesamiento de los datos**, cuando ya vamos a pasarle los datos de entrenamiento al método de Machine Learning. \n",
        "\n",
        "\n",
        "Para ilustrar los distintos ejemplos, vamos a trabajar con el mismo problema de clasificación: una versión reducida del problema [SUSY](https://archive.ics.uci.edu/ml/datasets/SUSY). Nos descargaremos los datos para trabajar con ellos. \n",
        "\n",
        "Posteriormente, convertiremos la variable de salida (clase) a entero. Después, uniremos las variables de entrada en una columna vector, usando para ello `vectorAssembler'. Finalmente, mostramos sólo estas dos columnas generadas, una con la variable de salida en formato numérico y otra con las variables de entrada agrupadas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox04GuFQpCqi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "617a0462-b401-4a4e-bc35-04df2adf8a4d"
      },
      "source": [
        "#Nos descargamos los ficheros de datos en Google Colab\n",
        "!wget -nv --no-check-certificate 'https://docs.google.com/uc?export=download&id=1HOrM49tCLA_NqHyD_ps_cv483FPN7aWo' -O susy-10k-tra.csv\n",
        "!wget -nv --no-check-certificate 'https://docs.google.com/uc?export=download&id=1HT80d5cwU7HMi2XK8CNgxgHvxRZEZB_d' -O susy-10k-tst.csv\n",
        "\n",
        "#Leemos los conjuntos de entrenamiento y test\n",
        "dfTra = spark.read.csv('susy-10k-tra.csv', inferSchema=True, header=True)\n",
        "dfTst = spark.read.csv('susy-10k-tst.csv', inferSchema=True, header=True)\n",
        "\n",
        "#Convertimos la variable clase en entera\n",
        "dfTra=dfTra.withColumn(\"clase\",dfTra.clase.cast(\"Integer\"))\n",
        "dfTst=dfTst.withColumn(\"clase\",dfTst.clase.cast(\"Integer\"))\n",
        "\n",
        "#Preprocesamos los datos para ser utilizados en un algoritmo de ML\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "#Unimos las variables de entrada con VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=dfTra.columns[:-1],outputCol=\"atributos\")\n",
        "train = assembler.transform(dfTra)\n",
        "train.select('atributos','clase').show(5)\n",
        "\n",
        "#Uso el mismo VectorAssemble para el conjunto de test (las columnas son iguales)\n",
        "test = assembler.transform(dfTst)\n",
        "test.select('atributos','clase').show(5)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-14 16:34:34 URL:https://doc-08-bc-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/kqfqq5gt2d8je0nd0v246a49op17h2re/1678811625000/11180625338828972622/*/1HOrM49tCLA_NqHyD_ps_cv483FPN7aWo?e=download&uuid=9f8171e3-ee02-443d-bf7d-f69a4eba4945 [3463157/3463157] -> \"susy-10k-tra.csv\" [1]\n",
            "2023-03-14 16:34:35 URL:https://doc-14-bc-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hhts6dj2o1g8ccfah9gphopq23aor3fq/1678811625000/11180625338828972622/*/1HT80d5cwU7HMi2XK8CNgxgHvxRZEZB_d?e=download&uuid=a30b0228-e976-4bc6-89ee-e75db81761e5 [3469204/3469204] -> \"susy-10k-tst.csv\" [1]\n",
            "+--------------------+-----+\n",
            "|           atributos|clase|\n",
            "+--------------------+-----+\n",
            "|[0.64334195852279...|    1|\n",
            "|[1.23746168613433...|    1|\n",
            "|[1.34331297874450...|    0|\n",
            "|[0.65310519933700...|    0|\n",
            "|[1.55166482925415...|    1|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+--------------------+-----+\n",
            "|           atributos|clase|\n",
            "+--------------------+-----+\n",
            "|[1.67953264713287...|    1|\n",
            "|[0.85768365859985...|    1|\n",
            "|[0.67941337823867...|    1|\n",
            "|[0.76688265800476...|    1|\n",
            "|[0.48874709010124...|    0|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSDoXaW_CL9q"
      },
      "source": [
        "##**3. TUBERÍAS DE SPARK (PIPELINES).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXCJ31nFC2Rm"
      },
      "source": [
        "Un proyecto de Ciencia de Datos normalmente implica preprocesar los datos (por ejemplo,  selección de variables de entrada, eliminación de ruido, balanceo de datos, preparar los datos para los algoritmos de aprendizaje,etc.) aprendizaje de un modelo y evaluación de los resultados. Por tanto, tenemos que realizar una **serie de transformaciones de los datos en secuencia**. \n",
        "\n",
        "Las [tuberías (pipelines)](https://spark.apache.org/docs/latest/ml-pipeline.html\n",
        " ) nos hacen más cómodo y optimizado combinar diferentes algoritmos en un sólo flujo de trabajo. Para trabajar con tuberías es necesario conocer algunos conceptos previos:\n",
        "\n",
        "* DataFrame: La estructura de datos con la que vamos a trabajar y que ya deberíamos conocer.\n",
        "\n",
        "* **[Transformador](http://spark.apache.org/docs/latest/ml-features.html)**: Un Transformador es un algoritmo que puede transformar un DataFrame en otro DataFrame. Por ejemplo, en la sección anterior hemos visto varias transformaciones para las variables categóricas. Otro ejemplo, sería un modelo Machine Learning que transforma un DataFrame con sólo variables de entrada en predicciones de la variable de salida. Un Transformador es una abstracción que incluye transformaciones sobre las variables o modelos aprendidos. Técnicamente, un Transformador implementa el método `transform()`, el cual convierte un DataFrame en otro, normalmente añadiendo una o más columnas.\n",
        "\n",
        "* **Estimador**: Un Estimador es un algoritmo que puede ser ajustado a un DataFrame (o aprendido a partir de él) para producir un Transformador. Por ejemplo, un algoritmo de aprendizaje es un Estimador que se entrena (usando el método `fit()`) con un DataFrame y produce un modelo. \n",
        "\n",
        "* **Tuberías**: Una tubería encadena múltiples Transformadores y Estimadores para especificar un flujo de trabajo de Machine Learning. Por ejemplo, podríamos añadir la indexación y el ensamblado de las variables de entrada en una tubería y luego aprender una regresión logística.\n",
        "\n",
        "* Parámetros: Todos los Transformadores y Estimadores comparten una interfaz común para especificar parámetros.\n",
        "\n",
        "\n",
        "Una tubería es una secuencia de etapas, donde cada etapa es un Transformador o un Estimador. Estas etapas se ejecutan en orden, y el DataFrame de entrada es modificado en cada etapa. Para etapas de Transformadores, el método `transform()` es invocado sobre el DataFrame. Para etapas de Estimadores, el método `fit()` es invocado para producir un Transformador. El método `transform()` es llamado en dicho Transformador y el DataFrame para realizar la transformación.\n",
        "\n",
        "\n",
        "Siguiendo el anterior ejemplo, podríamos haber hecho todas las transformaciones usando una tubería de la siguiente manera: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcRUOd7hGop8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01bd1e32-444a-42fb-bc27-d7f1d2c734af"
      },
      "source": [
        "#Unimos las variables de entrada con VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=dfTra.columns[:-1],outputCol=\"atributos\")\n",
        "\n",
        "#Añadimos el preprocesamiento a una tubería\n",
        "from pyspark.ml import Pipeline\n",
        "tuberiaParcial = Pipeline().setStages([assembler])\n",
        "\n",
        "#Construimos el modelo\n",
        "tuberia = tuberiaParcial.fit(dfTra)\n",
        "\n",
        "#Transformamos los datos y los mostramos\n",
        "datosPreprocesados = tuberia.transform(dfTra)\n",
        "datosPreprocesados.select('atributos','clase').show(5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|           atributos|clase|\n",
            "+--------------------+-----+\n",
            "|[0.64334195852279...|    1|\n",
            "|[1.23746168613433...|    1|\n",
            "|[1.34331297874450...|    0|\n",
            "|[0.65310519933700...|    0|\n",
            "|[1.55166482925415...|    1|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItbtlE6_IyCM"
      },
      "source": [
        "De esta forma, usando tuberías, se puede optimizar el preprocesamiento que se realizan sobre los datos, lo cual es muy importante cuando el volumen de datos es grande. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWA_MLhADGCi"
      },
      "source": [
        "##**4. TUBERÍAS CON MACHINE LEARNING.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZp4x101r72b"
      },
      "source": [
        "Como hemos visto, hemos incluido todo el preprocesamiento de nuestro conjunto de datos en una tubería. Ahora veremos como añadir también un algoritmo de Machine Learning. \n",
        "\n",
        "Seguiremos un ejemplo, concretamente de aprendizaje supervisado,  donde aprenderemos un clasificador de regresión logística a partir de los datos generados anteriormente.\n",
        "\n",
        "Siguiendo la terminología de Spark aprenderemos un modelo Transformador (que posee el método `transform()`) mediante un Estimador de regresión logística (que posee el método `fit()`). Es decir, aprenderemos un Estimador con los datos de entrenamiento y, el modelo aprendido lo usaremos como un Transformador sobre los datos de test. Dicho Transformador tomará los datos de test y los convertirá en una serie de predicciones. Dichas predicciones las usaremos para calcular la precisión del modelo. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKZQUkv3OZaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70d3a7c2-b952-4f27-c48e-2ed0c6bba3ac"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "#Aprendemos el Modelo de Regresión Logística\n",
        "rl = LogisticRegression(featuresCol = 'atributos', labelCol = 'clase', maxIter=10)\n",
        "rlModel = rl.fit(train)\n",
        "\n",
        "#Obtenemos las predicciones sobre el conjunto de test\n",
        "predicciones = rlModel.transform(test)\n",
        "\n",
        "#Ya podemos mostrar la precisión del modelo\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluador = MulticlassClassificationEvaluator(labelCol=\"clase\", metricName=\"accuracy\")\n",
        "print('Accuracy:', evaluador.evaluate(predicciones))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8043\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hezOwjPXp5Nz"
      },
      "source": [
        "El anterior ejemplo, lo podemos mejorar. Ahora lo que haremos será añadir a la tubería el modelo de regresión logística, de esta forma conseguimos que Spark optimice más aún el proceso. \n",
        "\n",
        "Definiremos la tubería y aprenderemos el modelo. Dicho modelo, con el preprocesado de datos y el clasificador, se aplicará al conjunto de test para hacer las predicciones. Con estas predicciones podemos calcular la proporción de bien clasificados.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz8ilnaMmDk-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3c0d2c2-f364-40bf-d56b-1c718f446e75"
      },
      "source": [
        "#Unimos las variables de entrada con VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=dfTra.columns[:-1],outputCol=\"atributos\")\n",
        "\n",
        "#Incorporamos el modelo de regresión logística\n",
        "rl = LogisticRegression(featuresCol = 'atributos', labelCol = 'clase', maxIter=10)\n",
        "\n",
        "#Añadimos las etapas a una tubería\n",
        "etapas=[assembler,rl]\n",
        "modeloRL = Pipeline().setStages(etapas)\n",
        "\n",
        "#Construimos el modelo: preprocesamiento + regresión logística\n",
        "clasificador = modeloRL.fit(dfTra)\n",
        "\n",
        "#Obtenemos las predicciones sobre el conjunto de test \n",
        "predicciones = clasificador.transform(dfTst)\n",
        "\n",
        "#Ya podemos mostrar la precisión del modelo\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"clase\", metricName=\"accuracy\")\n",
        "print('Accuracy:', evaluator.evaluate(predicciones))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8043\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFNjux2udLGh"
      },
      "source": [
        "Veamos a continuación las métricas y modelos de validación que podemos usar en Spark, pero nos centraremos en el aprendizaje supervisado. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-t1BW3nDlco"
      },
      "source": [
        "##**5. MÉTRICAS PARA LA EVALUACIÓN DE LOS MODELOS DE APRENDIZAJE SUPERVISADO.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhbV_xKD_9e1"
      },
      "source": [
        "Dentro de las herramientas de aprendizaje supervisado, tenemos clasificadores y regresores, dependiendo de si la variable que queremos predecir es discreta o continua, respectivamente.\n",
        "\n",
        "En los clasificadores, distinguiremos aquellos que son para **clasificación binaria**, es decir, la variable de salida sólo tiene dos estados, de la **clasificación multiclase**, es decir, aquellos en los que la variable de salida tiene más de dos estados.\n",
        "\n",
        "Tengamos en cuenta, que los clasificadores que nos permiten trabajar con problemas multiclase también nos permiten trabajar con problemas binarios. Lo cual tiene sentido, es decir,  si un algoritmo es capaz de discriminar entre 4 ó 10 valores de la variable de salida, también debería ser capaz de distinguir entre dos.\n",
        "\n",
        "No obstante, también podemos usar clasificadores binarios en problemas multiclase, por ejemplo, utilizando la metodología Uno-contra-todos.  Este esquema construye un clasificador base para cada valor de la variable de salida y dicho clasificador distingue el valor i-ésimo del resto. \n",
        "\n",
        "\n",
        "Primero, nos vamos a centrar en el caso de los problemas de clasificación binaria. En estos clasificadores vamos a usar la clase `BinaryClassificationEvaluator` que nos permite calcular dos medidas:\n",
        "\n",
        "1.   `areaUnderROC` : El área bajo la curva ROC, o también ROC AUC. \n",
        "2.   `areaUnderPR`: Es el área bajo la curva  Precision-Recall, o también Precision-Recall AUC. La curva PR es el resultado de dibujar la gráfica entre el precision y el recall.  \n",
        "\n",
        "<!-- Esta gráfica nos permite ver a partir de qué recall tenemos una degradación de la precisión y viceversa. Lo ideal sería una curva que se acerque lo máximo posible a la esquina superior derecha (alta precisión y alto recall). Esta métrica es más apropiada para conjuntos no balanceados, es decir, hay un reparto desigual acusado entre las dos clases del problema.\n",
        "\n",
        "\n",
        "Vamos a usar el ejemplo anterior para mostrar ambas métricas:\n",
        "\n",
        " #La regresión logística, también nos permite mostras la curvas ROC\n",
        "import matplotlib.pyplot as plt\n",
        "trainingSummary = clasificador.stages[-1].summary\n",
        "roc = trainingSummary.roc.toPandas()\n",
        "plt.plot(roc['FPR'].tolist(),roc['TPR'].tolist(),color='orange')\n",
        "plt.ylabel('False Positive Rate')\n",
        "plt.xlabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.plot([0, 1], [0, 1], color='navy',  linestyle='--')\n",
        "plt.show()\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVOUTyviI0kb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f81c6c04-900a-4e90-da77-282e41cb1239"
      },
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "#Ya podemos mostrar la bondad de las predicciones del modelo cn AUC\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"clase\", metricName=\"areaUnderROC\")\n",
        "print('ROC AUC:', evaluator.evaluate(predicciones))\n",
        "print ('areaUnderPR', evaluator.evaluate(predicciones, {evaluator.metricName: \"areaUnderPR\"}) )"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC AUC: 0.8528537606105937\n",
            "areaUnderPR 0.8012755689164685\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YzRvblHI1uW"
      },
      "source": [
        "Veamos ahora el caso, de la clasificación multiclase. Hay que señalar que las métricas que usan cuando hay más de una clase se pueden usar igualmente en clasificación binaria.  En este caso vamos a usar la clase `MulticlassClassificationEvaluator` y las medidas que podemos obtener son:\n",
        "*  `accuracy`: También llamado proporción de bien clasificados. Se refiere a la ratio de ejemplos positivos correctamente identificados con respecto al total. \n",
        "*  `weightedPrecision`: Es la precisión promediada por etiqueta, es decir, se calcula la media de la precisión por cada etiqueta.\n",
        "*  `weightedRecall`: Es el *Recall* promediado por etiqueta, es decir, se calcula la media de la precisión por cada etiqueta.\n",
        "*  `f1`: También denominado *F-measure*, es la métrica que combina *precision* y  *recall*.\n",
        "\n",
        "Sigamos con el ejemplo anterior que, aún siendo un problema de clasificación binario, podemos calcular estas medidas:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_q_5O4FRHej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a63a6c2-a4c7-452f-8145-bfd94d6d7363"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "#Ya podemos mostrar la bondad de las predicciones del modelo multiclase\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"clase\")\n",
        "print('F1:', evaluator.evaluate(predicciones))\n",
        "print('Accuracy:', evaluator.evaluate(predicciones, {evaluator.metricName: \"accuracy\"}))\n",
        "print('Weighted Precision:', evaluator.evaluate(predicciones, {evaluator.metricName: \"weightedPrecision\"}))\n",
        "print('Weighted Recall:', evaluator.evaluate(predicciones, {evaluator.metricName: \"weightedRecall\"}))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.8039722690022877\n",
            "Accuracy: 0.8043\n",
            "Weighted Precision: 0.803679034355548\n",
            "Weighted Recall: 0.8043\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzPE5-RPRINc"
      },
      "source": [
        "\n",
        "\n",
        "Cuando la variable a predecir es una variable continua, se dice que el modelo Machine Learning es un regresor. Para regresión vamos a usar la clase `RegressionEvaluator` y las medidas vistas en el módulo 4 que podemos obtener son:\n",
        "* `mse`: Error cuadrático medio. Mide el error cuadrado promedio de nuestras predicciones. Para cada punto, calcula la diferencia cuadrada entre las predicciones y el objetivo y luego promedia esos valores.\n",
        "* `rmse`:  Es la raíz cuadrada del error cuadrático medio. La raíz cuadrada se introduce para hacer que la escala de los errores sea igual a la escala de los objetivos.\n",
        "\n",
        "* `r2`:  El coeficiente de determinación, o $r^2$, está estrechamente relacionada con el error cuadrático medio, pero siempre estará entre -∞ y 1.\n",
        "* `mae`: Error absoluto medio. El error se calcula como un promedio de diferencias absolutas entre los valores objetivo y las predicciones. De esta forma, todas las diferencias individuales se ponderan por igual en el promedio.\n",
        "\n",
        "\n",
        "Sigamos con el ejemplo anterior, pero al SER un problema de clasificación binario, no van a tener mucho sentido los resultados. Sólo nos sirve para ver cómo se usa:\n",
        "<!-- from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "#Ya podemos mostrar la bondad de las predicciones del modelo multiclase\n",
        "evaluator = RegressionEvaluator(labelCol=\"clase\")\n",
        "print('rmse:', evaluator.evaluate(predicciones))\n",
        "print('mse:', evaluator.evaluate(predicciones, {evaluator.metricName: \"mse\"}))\n",
        "print('r2:', evaluator.evaluate(predicciones, {evaluator.metricName: \"r2\"}))\n",
        "print('mae:', evaluator.evaluate(predicciones, {evaluator.metricName: \"mae\"}))-->\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwXHUHAuc6Ux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4ebcb36-8e1e-4aa2-fa65-6759e0898f30"
      },
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "#Ya podemos mostrar la bondad de las predicciones de un regresor \n",
        "evaluator = RegressionEvaluator(labelCol=\"clase\")\n",
        "print('rmse:', evaluator.evaluate(predicciones))\n",
        "print('r2:', evaluator.evaluate(predicciones, {evaluator.metricName: \"r2\"}))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rmse: 0.4423799272118933\n",
            "r2: 0.12360226428573085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPbV3-cMEJUO"
      },
      "source": [
        "##**6. VALIDACIÓN DE LOS MODELOS DE APRENDIZAJE SUPERVISADO.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHQrAPCwVunj"
      },
      "source": [
        "Hasta ahora, hemos validado nuestro modelo de ejemplo con diversas métricas, pero usando un sólo método: Entrenamiento y Test. Aunque ya teníamos los datos partidos, se puede automatizar este proceso a partir de un sólo conjunto de datos. Podemos usar la clase `TrainValidationSplit`, indicándole el modelo del clasificador, un objeto `ParamGrimBuilder` que se utiliza en otras funcionalidades que veremos más adelante, la clase que se encarga de la evaluación y finalmente la proporción de la partición. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eBkvoX_-V6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56da6be3-baac-4f47-faad-fbb24a241fdb"
      },
      "source": [
        "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
        "\n",
        "#Uno los conjuntos de entrenamiento y test en un dataframe\n",
        "df=dfTra.union(dfTst)\n",
        "\n",
        "#Creamos una clase Evaluador\n",
        "evaluador = MulticlassClassificationEvaluator(labelCol=\"clase\", metricName=\"accuracy\")\n",
        "\n",
        "#Usamos la regresión logística para ser evaluada \n",
        "validadorTT= TrainValidationSplit(estimator=modeloRL,\n",
        "                           estimatorParamMaps=ParamGridBuilder().build(),\n",
        "                           evaluator=evaluador,\n",
        "                           trainRatio=0.8) # 80% de los datos para entrenamiento\n",
        "validadorTT.setSeed(2022)\n",
        "#Entrenamos el modelo con el conjunto completo de datos\n",
        "modeloTT=validadorTT.fit(df)\n",
        "\n",
        "#Mostramos la precisión del modelo\n",
        "print(modeloTT.getEvaluator().getMetricName(), ':',modeloTT.validationMetrics[0] )"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy : 0.78626524010948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-foeAW7Qb_Cq"
      },
      "source": [
        "\n",
        "Este tipo de validación es bastante sencillo y rápido, aunque tiene poco rigor estadístico y los resultados dependen demasiado de cómo hagamos la partición de los datos. A modo de curiosidad, si la semilla aleatoria elegida hubiese sido `seed=2020`, en lugar de `seed=2022`, los resultados en Accuracy hubieran sido un 1% mejores.\n",
        "\n",
        "Al ser una validación tan sesgada, es preferible hacer una validación cruzada de, al menos, unas 10 hojas. No obstante, en Big Data, rara vez podremos permitirnos el lujo de escoger una validación tan costosa.\n",
        "\n",
        "Para esta validación podemos usar la clase `CrossValidator`, y donde antes indicábamos el porcentaje de datos para entrenamiento, ahora indicaremos el número de hojas. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNELCN_QAqSh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "725b825f-7041-496d-d297-4a64b4c809db"
      },
      "source": [
        "from pyspark.ml.tuning import CrossValidator\n",
        "\n",
        "#Validación cruzada\n",
        "crossval = CrossValidator(estimator=modeloRL,\n",
        "                          estimatorParamMaps=ParamGridBuilder().build(),\n",
        "                          evaluator=evaluador,\n",
        "                          numFolds=10) \n",
        "#Entrenamos el modelo\n",
        "cvModel=crossval.fit(df)\n",
        "\n",
        "#Ya podemos mostrar la precisión del modelo\n",
        "print(cvModel.getEvaluator().getMetricName(), ':',cvModel.avgMetrics[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy : 0.7915715430606218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmVuiYakyIqM"
      },
      "source": [
        "##**7. AJUSTE DE LOS HIPERPARÁMETROS DE LOS MODELOS.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T23_qVUkDWvx"
      },
      "source": [
        "Los algoritmos de aprendizaje automático aceptan una serie de parámetros que afectan a su funcionamiento. Estos parámetros de los modelos se denominan **hiperparámetros**. Por ejemplo, un número demasiado alto de árboles en un RandomForest puede hacer que tarde demasiado tiempo o que sobreajuste, mientras que un número demasiado bajo provocará una menor capacidad predictora. \n",
        "\n",
        "Por tanto, es útil encontrar los mejores valores de los hiperparámetros para que el modelo aprenda de una manera óptima el problema a resolver.\n",
        "\n",
        "Aunque lo hemos visto en el apartado anterior, en realidad, Spark no proporciona ningún tipo de validación de los modelos. Hemos usado los dos tipos de validación existentes para el ajuste de los hiper parámetros.\n",
        "\n",
        "En el ajuste de los hiperparámetros, tenemos que indicar que conjunto de valores pueden tomar cada parámetro usando la clase ` ParamGridBuilder`. Lo que hicimos en el apartado anterior era dejar esos conjuntos de valores vacíos y así tomaba sólo los que hubieramos indicado o sus valores por defecto.\n",
        "\n",
        "Para hacer el ajuste de los hiperparámetros, vamos a indicar con dicha clase un conjunto de valores que puede tomar, por ejemplo, la regresión logística. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLLc1bWIgxpj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27ae5b6a-a4b0-438e-aa9b-f4e4d5a5af9a"
      },
      "source": [
        "from pyspark.ml.tuning import CrossValidator\n",
        "\n",
        "# Usamos ParamGridBuilder para cosntruir un conjunto de parámetros sobre los que\n",
        "# hacer una búsqueda. Con 3 valores para rl.maxIter y 2 valores para rl.regParam,\n",
        "# tendremos 3 x 2 = 6 configuraciones de hiperparámetros distintas.\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(rl.maxIter , [10, 100, 1000]) \\\n",
        "    .addGrid(rl.regParam, [0.1, 0.01]) \\\n",
        "    .build()\n",
        "\n",
        "\n",
        "#Validación cruzada\n",
        "crossval = CrossValidator(estimator=modeloRL,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=evaluador,\n",
        "                          numFolds=10) \n",
        "\n",
        "#Entrenamos el modelo con el conjunto de entrenamiento\n",
        "cvModel=crossval.fit(dfTra)\n",
        "\n",
        "#Mostramos las distintas evaluaciones calculadas. \n",
        "print(cvModel.getEvaluator().getMetricName(), ':',cvModel.avgMetrics)\n",
        "\n",
        "#Utilizamos el conjunto de test para medir como de bueno es el modelo. Para \n",
        "# ello usamos la mejor configuración (bestModel) de los parámetros encontrada.\n",
        "predicciones = cvModel.bestModel.transform(dfTst)\n",
        "\n",
        "#Ya podemos mostrar la bondad del modelo\n",
        "print('Accuracy:', evaluador.evaluate(predicciones))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy : [0.7577256434658226, 0.7712002724930181, 0.7576249385312808, 0.7718680605128679, 0.7576249385312808, 0.7718680605128679]\n",
            "Accuracy: 0.8035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUbziVwFj1-e"
      },
      "source": [
        "Observemos un detalle del ejemplo anterior, para entrenar el modelo hemos usado el conjunto de entrenamiento. Para evaluar la bondad del modelo usamos el conjunto de test. Téngase en cuenta que podríamos tener un resultado sobreajustado si nos quedamos con el mejor resultado de la búsqueda. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHtaTrToin7P"
      },
      "source": [
        "##**8.REFERENCIAS BIBLIOGRÁFICAS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ldzM6rqqoNm"
      },
      "source": [
        "* The Apache Software Foundation. \"Machine Learning Library (MLlib) Guide\". (2020). [Acceso 9 de junio de 2020]. Disponible en: https://spark.apache.org/docs/latest/ml-guide.html\n",
        "\n",
        "* The Apache Software Foundation. \"Pyspark.ml package API\". (2020). [Acceso 9 de junio de 2020]. Disponible en: https://spark.apache.org/docs/latest/api/python/pyspark.ml.html\n",
        "\n",
        "* The Apache Software Foundation. \"Extracting, transforming and selecting features\". (2020). [Acceso 9 de junio de 2020]. Disponible en: http://spark.apache.org/docs/latest/ml-features.html\n",
        "\n",
        "* The Apache Software Foundation. \"ML Pipelines\". (2020). [Acceso 9 de junio de 2020]. Disponible en: https://spark.apache.org/docs/latest/ml-pipeline.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHPSg3eUi75t"
      },
      "source": [
        "###**Referencias Adicionales**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfcnM4oeEqq9"
      },
      "source": [
        "* Lakshay Arora. \"Want to Build Machine Learning Pipelines? A Quick Introduction using PySpark\". (2019). [Acceso 9 de junio de 2020]. Disponible en: https://www.analyticsvidhya.com/blog/2019/11/build-machine-learning-pipelines-pyspark/\n",
        "\n",
        "* Susan Li. \"Machine Learning with PySpark and MLlib — Solving a Binary Classification Problem\". (2018). [Acceso 9 de junio de 2020]. Disponible en:  https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa\n",
        "\n",
        "* The Apache Software Foundation. \"Evaluation Metrics - RDD-based API\". (2020). [Acceso 9 de junio de 2020]. Disponible en: https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html\n",
        "\n",
        "\n",
        "* The Apache Software Foundation. \"ML Tuning: model selection and hyperparameter tuning\". (2020). [Acceso 9 de junio de 2020]. Disponible en: https://spark.apache.org/docs/latest/ml-tuning.html#ml-tuning-model-selection-and-hyperparameter-tuning\n"
      ]
    }
  ]
}